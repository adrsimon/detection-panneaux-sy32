{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49fc1aff-98d3-4eee-bef0-2c98dc3fa03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de panneaux 'frouge' : 81\n",
      "Nombre de panneaux 'ceder' : 120\n",
      "Nombre de panneaux 'interdiction' : 289\n",
      "Nombre de panneaux 'fvert' : 94\n",
      "Nombre de panneaux 'stop' : 99\n",
      "Nombre de panneaux 'danger' : 155\n",
      "Nombre de panneaux 'obligation' : 117\n",
      "Nombre de panneaux 'forange' : 54\n",
      "Nombre de panneaux 'ff' : 10\n",
      "Nombre de panneaux 'Stop' : 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "def compter_panneaux(images_path, labels_path):\n",
    "    # Dictionnaire pour compter les panneaux\n",
    "    panneaux_count = {}\n",
    "\n",
    "    # Liste des fichiers images\n",
    "    image_files = [f for f in os.listdir(images_path) if f.endswith('.jpg')]\n",
    "\n",
    "    # Parcourir chaque image\n",
    "    for image_file in image_files:\n",
    "        # Obtenir le nom de base sans l'extension\n",
    "        base_name = os.path.splitext(image_file)[0]\n",
    "        \n",
    "        # Chemin vers le fichier label correspondant\n",
    "        label_file = os.path.join(labels_path, base_name + '.csv')\n",
    "        \n",
    "        # Vérifier si le fichier label existe et n'est pas vide\n",
    "        if os.path.exists(label_file) and os.path.getsize(label_file) > 0:\n",
    "            # Lire le fichier CSV\n",
    "            with open(label_file, 'r') as csvfile:\n",
    "                csvreader = csv.reader(csvfile)\n",
    "                # Parcourir chaque ligne du fichier CSV\n",
    "                for row in csvreader:\n",
    "                    if len(row) >= 5:  # Vérifier que la ligne a au moins 5 colonnes\n",
    "                        # Supposons que la classe du panneau est dans la 5ème colonne (index 4)\n",
    "                        class_name = row[4]\n",
    "                        if class_name in panneaux_count:\n",
    "                            panneaux_count[class_name] += 1\n",
    "                        else:\n",
    "                            panneaux_count[class_name] = 1\n",
    "\n",
    "    # Afficher le résultat\n",
    "    for panneau, count in panneaux_count.items():\n",
    "        print(f\"Nombre de panneaux '{panneau}' : {count}\")\n",
    "\n",
    "# Chemin vers le dossier des images et des labels\n",
    "images_path = 'train/images'\n",
    "labels_path = 'train/labels'\n",
    "\n",
    "# Appeler la fonction\n",
    "compter_panneaux(images_path, labels_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5810659-8fbf-4ff1-a225-92bf93cc5717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de fichiers CSV 'empty' : 54\n",
      "Nombre de panneaux 'frouge' : 81\n",
      "Nombre de panneaux 'ceder' : 120\n",
      "Nombre de panneaux 'interdiction' : 289\n",
      "Nombre de panneaux 'fvert' : 94\n",
      "Nombre de panneaux 'stop' : 100\n",
      "Nombre de panneaux 'danger' : 155\n",
      "Nombre de panneaux 'obligation' : 117\n",
      "Nombre de panneaux 'empty' : 54\n",
      "Nombre de panneaux 'forange' : 54\n",
      "Nombre de panneaux 'ff' : 10\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "def add_empty_label_to_empty_csv_files(labels_path):\n",
    "    # Liste des fichiers CSV dans le répertoire des labels\n",
    "    csv_files = [f for f in os.listdir(labels_path) if f.endswith('.csv')]\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        csv_path = os.path.join(labels_path, csv_file)\n",
    "        # Vérifie si le fichier est vide ou n'a que des nouvelles lignes\n",
    "        with open(csv_path, 'r') as file:\n",
    "            content = file.read().strip()\n",
    "\n",
    "        if content == \"\" or content == \"empty\":\n",
    "            with open(csv_path, 'w', newline='') as csvfile:\n",
    "                csvwriter = csv.writer(csvfile)\n",
    "                # Ajoute l'entrée \"empty\" dans le fichier CSV vide\n",
    "                csvwriter.writerow([\"X\", \"X\", \"X\", \"X\", \"empty\"])\n",
    "        else:\n",
    "            # Vérifie et corrige l'annotation \"Stop\"\n",
    "            with open(csv_path, 'r') as csvfile:\n",
    "                csvreader = csv.reader(csvfile)\n",
    "                rows = list(csvreader)\n",
    "                for row in rows:\n",
    "                    if len(row) > 4 and row[4] == \"Stop\":\n",
    "                        row[4] = \"stop\"\n",
    "\n",
    "            # Écrit les lignes corrigées dans le fichier CSV\n",
    "            with open(csv_path, 'w', newline='') as csvfile:\n",
    "                csvwriter = csv.writer(csvfile)\n",
    "                csvwriter.writerows(rows)\n",
    "\n",
    "# Appeler la fonction pour ajouter l'étiquette \"empty\"\n",
    "add_empty_label_to_empty_csv_files(labels_path)\n",
    "\n",
    "def count_empty_labels(labels_path):\n",
    "    empty_count = 0\n",
    "\n",
    "    # Liste des fichiers CSV dans le répertoire des labels\n",
    "    csv_files = [f for f in os.listdir(labels_path) if f.endswith('.csv')]\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        csv_path = os.path.join(labels_path, csv_file)\n",
    "        with open(csv_path, 'r') as csvfile:\n",
    "            csvreader = csv.reader(csvfile)\n",
    "            rows = list(csvreader)\n",
    "            # Vérifie si le fichier ne contient que l'étiquette \"empty\"\n",
    "            if len(rows) == 1 and rows[0][4] == \"empty\":\n",
    "                empty_count += 1\n",
    "\n",
    "    return empty_count\n",
    "\n",
    "# Chemin vers le dossier des labels\n",
    "labels_path = 'train/labels2'\n",
    "\n",
    "# Compter le nombre de fichiers CSV contenant l'étiquette \"empty\"\n",
    "empty_files_count = count_empty_labels(labels_path)\n",
    "print(f\"Nombre de fichiers CSV 'empty' : {empty_files_count}\")\n",
    "\n",
    "images_path2 = 'train/images2'\n",
    "labels_path2 = 'train/labels2'\n",
    "compter_panneaux(images_path2, labels_path2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5cb0e3-e665-4fe3-b7a6-17973411d3d5",
   "metadata": {},
   "source": [
    "\n",
    "def clear_empty_label_files(labels_path):\n",
    "    # Liste des fichiers CSV dans le répertoire des labels\n",
    "    csv_files = [f for f in os.listdir(labels_path) if f.endswith('.csv')]\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        csv_path = os.path.join(labels_path, csv_file)\n",
    "        # Vérifie si le fichier est vide ou n'a que des nouvelles lignes\n",
    "        with open(csv_path, 'r') as file:\n",
    "            content = file.read().strip()\n",
    "        if  content == \"empty\":\n",
    "            with open(csv_path, 'w', newline='') as csvfile:\n",
    "                 csvfile.truncate()\n",
    "\n",
    "clear_empty_label_files(labels_path)\n",
    "\n",
    "# Chemin vers le dossier des labels\n",
    "labels_path = 'train/labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "219d3e77-a534-428b-ba32-c537d577bc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'images produites pour 'frouge' : 24\n",
      "Nombre d'images produites pour 'ceder' : 48\n",
      "Nombre d'images produites pour 'fvert' : 44\n",
      "Nombre d'images produites pour 'stop' : 68\n",
      "Nombre d'images produites pour 'forange' : 34\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image, ImageEnhance, ImageOps\n",
    "\n",
    "def add_noise(img, noise_level=0.05):\n",
    "    \"\"\"Add random noise to an image.\"\"\"\n",
    "    np_img = np.array(img)\n",
    "    noise = np.random.normal(loc=0, scale=noise_level, size=np_img.shape)\n",
    "    np_img = np.clip(np_img + noise * 255, 0, 255).astype(np.uint8)\n",
    "    return Image.fromarray(np_img)\n",
    "\n",
    "def rotate_image_left(img):\n",
    "    \"\"\"Rotate the image 90 degrees to the left (counter-clockwise).\"\"\"\n",
    "    return img.transpose(Image.ROTATE_90)\n",
    "\n",
    "def adjust_and_save_image(image_path, csv_path, new_image_path, new_csv_path, transformation):\n",
    "    \"\"\"Create an adjusted image and save it with a new label CSV.\"\"\"\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "    # Apply the specified transformation\n",
    "    if transformation == \"rotate_left\":\n",
    "        img = rotate_image_left(img)\n",
    "    elif transformation == \"flip_and_noise\":\n",
    "        img = rotate_image_left(img)  # Mirroring effect\n",
    "        img = add_noise(img)\n",
    "\n",
    "    img.save(new_image_path)\n",
    "\n",
    "    # Copy the CSV file with the new name\n",
    "    shutil.copy(csv_path, new_csv_path)\n",
    "\n",
    "def balance_dataset(images_path, labels_path, new_images_path):\n",
    "    # Create new images and labels directories if they don't exist\n",
    "    new_images_dir = os.path.join(new_images_path, 'images')\n",
    "    new_labels_dir = os.path.join(new_images_path, 'labels')\n",
    "    os.makedirs(new_images_dir, exist_ok=True)\n",
    "    os.makedirs(new_labels_dir, exist_ok=True)\n",
    "\n",
    "    # List of types to augment\n",
    "    types_to_augment = ['frouge', 'ceder', 'fvert', 'stop', 'forange']\n",
    "    types_to_flip_and_noise = ['frouge', 'fvert', 'forange']\n",
    "\n",
    "    # Count the number of each type of panel and keep track of produced images\n",
    "    panneaux_count = {type_name: 0 for type_name in types_to_augment}\n",
    "    image_files = [f for f in os.listdir(images_path) if f.endswith('.jpg')]\n",
    "\n",
    "    for image_file in image_files:\n",
    "        base_name = os.path.splitext(image_file)[0]\n",
    "        label_file = os.path.join(labels_path, base_name + '.csv')\n",
    "        already_augmented = False\n",
    "\n",
    "        if os.path.exists(label_file) and os.path.getsize(label_file) > 0:\n",
    "            with open(label_file, 'r') as csvfile:\n",
    "                csvreader = csv.reader(csvfile)\n",
    "                rows = list(csvreader)\n",
    "\n",
    "                # Check if the image contains a unique panel of a type we want to augment\n",
    "                if len(rows) == 1 and rows[0][4] in types_to_augment:\n",
    "                    class_name = rows[0][4]\n",
    "\n",
    "                    # Perform left rotation augmentation\n",
    "                    new_base_name = f\"{base_name}_aug_left\"\n",
    "                    new_image_path = os.path.join(new_images_dir, new_base_name + '.jpg')\n",
    "                    new_csv_path = os.path.join(new_labels_dir, new_base_name + '.csv')\n",
    "                    adjust_and_save_image(os.path.join(images_path, image_file), label_file, new_image_path, new_csv_path, \"rotate_left\")\n",
    "                    panneaux_count[class_name] += 1\n",
    "                    already_augmented = True\n",
    "\n",
    "                    # Perform flip and noise augmentation if applicable\n",
    "                    if class_name in types_to_flip_and_noise:\n",
    "                        new_base_name = f\"{base_name}_aug_flip_noise\"\n",
    "                        new_image_path = os.path.join(new_images_dir, new_base_name + '.jpg')\n",
    "                        new_csv_path = os.path.join(new_labels_dir, new_base_name + '.csv')\n",
    "                        adjust_and_save_image(os.path.join(images_path, image_file), label_file, new_image_path, new_csv_path, \"flip_and_noise\")\n",
    "                        panneaux_count[class_name] += 1\n",
    "\n",
    "        # Perform final augmentation if not already augmented and does not contain 'interdiction'\n",
    "        if not already_augmented:\n",
    "            with open(label_file, 'r') as csvfile:\n",
    "                csvreader = csv.reader(csvfile)\n",
    "                rows = list(csvreader)\n",
    "                contains_interdiction = any(row[4] == 'interdiction' for row in rows)\n",
    "                if not contains_interdiction:\n",
    "                    new_base_name = f\"{base_name}_aug_final\"\n",
    "                    new_image_path = os.path.join(new_images_dir, new_base_name + '.jpg')\n",
    "                    new_csv_path = os.path.join(new_labels_dir, new_base_name + '.csv')\n",
    "                    adjust_and_save_image(os.path.join(images_path, image_file), label_file, new_image_path, new_csv_path, \"flip_and_noise\")\n",
    "\n",
    "        # Copy original images and labels to the new directories\n",
    "        shutil.copy(os.path.join(images_path, image_file), os.path.join(new_images_dir, image_file))\n",
    "        shutil.copy(label_file, os.path.join(new_labels_dir, base_name + '.csv'))\n",
    "\n",
    "    # Print the result\n",
    "    for panneau, count in panneaux_count.items():\n",
    "        print(f\"Nombre d'images produites pour '{panneau}' : {count}\")\n",
    "\n",
    "# Définir les chemins\n",
    "images_path = 'train/images2'\n",
    "labels_path = 'train/labels2'\n",
    "new_images_path = 'Newimages'\n",
    "\n",
    "# Appeler la fonction pour équilibrer le dataset\n",
    "balance_dataset(images_path, labels_path, new_images_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ffc35187-8e90-4d35-8046-61d6069a89ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de panneaux 'frouge' : 81\n",
      "Nombre de panneaux 'ceder' : 120\n",
      "Nombre de panneaux 'interdiction' : 289\n",
      "Nombre de panneaux 'fvert' : 94\n",
      "Nombre de panneaux 'stop' : 100\n",
      "Nombre de panneaux 'danger' : 155\n",
      "Nombre de panneaux 'obligation' : 117\n",
      "Nombre de panneaux 'empty' : 54\n",
      "Nombre de panneaux 'forange' : 54\n",
      "Nombre de panneaux 'ff' : 10\n",
      "\n",
      "Nombre de panneaux 'frouge' : 161\n",
      "Nombre de panneaux 'ceder' : 211\n",
      "Nombre de panneaux 'interdiction' : 289\n",
      "Nombre de panneaux 'fvert' : 188\n",
      "Nombre de panneaux 'stop' : 176\n",
      "Nombre de panneaux 'danger' : 247\n",
      "Nombre de panneaux 'obligation' : 215\n",
      "Nombre de panneaux 'empty' : 108\n",
      "Nombre de panneaux 'forange' : 111\n",
      "Nombre de panneaux 'ff' : 15\n"
     ]
    }
   ],
   "source": [
    "images_path3 = 'Newimages/images'\n",
    "labels_path3 = 'Newimages/labels'\n",
    "compter_panneaux(images_path, labels_path)\n",
    "print(\"\")\n",
    "compter_panneaux(images_path3, labels_path3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2078ba0d-5244-444d-8c7a-97c52cd59558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from skimage.feature import hog\n",
    "from sklearn import svm\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def extract_hog_features_single_image(image):\n",
    "    \"\"\"Extract HOG features from a single image.\"\"\"\n",
    "    image = image.convert('L')  # Convert to grayscale\n",
    "    image = image.resize((256, 256))  # Resize to 256x256\n",
    "    image = np.array(image)\n",
    "    features, _ = hog(image, orientations=9, pixels_per_cell=(8, 8),\n",
    "                      cells_per_block=(2, 2), block_norm='L2-Hys',\n",
    "                      visualize=True, transform_sqrt=True)\n",
    "    return features\n",
    "\n",
    "def load_data(images_path, labels_path):\n",
    "    \"\"\"Load images and labels from the specified directories.\"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    image_files = [f for f in os.listdir(images_path) if f.endswith('.jpg')]\n",
    "    for image_file in image_files:\n",
    "        image_path = os.path.join(images_path, image_file)\n",
    "        label_file = os.path.join(labels_path, os.path.splitext(image_file)[0] + '.csv')\n",
    "\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        with open(label_file, 'r') as csvfile:\n",
    "            csvreader = csv.reader(csvfile)\n",
    "            panel_labels = set()\n",
    "            for row in csvreader:\n",
    "                if row[4] != 'empty':  # Skip images labeled as 'empty'\n",
    "                    panel_labels.add(row[4])\n",
    "            if panel_labels:\n",
    "                images.append(image)\n",
    "                labels.append(panel_labels)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "def extract_hog_features_parallel(images):\n",
    "    \"\"\"Extract HOG features from a list of images in parallel.\"\"\"\n",
    "    hog_features = Parallel(n_jobs=-1)(delayed(extract_hog_features_single_image)(image) for image in images)\n",
    "    return hog_features\n",
    "\n",
    "images_path = 'Newimages/images'\n",
    "labels_path = 'Newimages/labels'\n",
    "images, labels = load_data(images_path, labels_path)\n",
    "\n",
    "# Convertir les ensembles de labels en listes\n",
    "labels_list = [list(label_set) for label_set in labels]\n",
    "\n",
    "# Extraire les descripteurs HOG en parallèle\n",
    "hog_features = extract_hog_features_parallel(images)\n",
    "\n",
    "# Convert list of features to a numpy array\n",
    "hog_features = np.array(hog_features)\n",
    "\n",
    "# Encoder les étiquettes\n",
    "le = LabelEncoder()\n",
    "labels_encoded = [le.fit_transform(label_set) for label_set in labels_list]\n",
    "\n",
    "# Flatten the labels_encoded for fitting\n",
    "flattened_labels = [item for sublist in labels_encoded for item in sublist]\n",
    "flattened_hog_features = [hog_features[i] for i, sublist in enumerate(labels_encoded) for _ in sublist]\n",
    "\n",
    "# Convert flattened_hog_features to a numpy array\n",
    "flattened_hog_features = np.array(flattened_hog_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "617534e6-ad75-4b48-b918-c1248e1c5fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80.96%\n"
     ]
    }
   ],
   "source": [
    "# Entraîner un SVM avec One-vs-Rest\n",
    "svm_clf = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True))\n",
    "svm_clf.fit(flattened_hog_features, flattened_labels)\n",
    "\n",
    "# Évaluer le modèle sur les données d'entraînement\n",
    "predicted_labels = svm_clf.predict(flattened_hog_features)\n",
    "accuracy = accuracy_score(flattened_labels, predicted_labels)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dd2df9b6-6450-401b-a8ba-a6207a4fe6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 1 - Predicted: {'fvert'}, Actual: {'frouge', 'interdiction'}\n",
      "Image 2 - Predicted: {'fvert'}, Actual: {'ceder', 'obligation'}\n",
      "Image 3 - Predicted: {'fvert'}, Actual: {'stop'}\n",
      "Image 4 - Predicted: {'fvert'}, Actual: {'frouge', 'interdiction', 'obligation'}\n",
      "Image 5 - Predicted: {'fvert'}, Actual: {'interdiction'}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Predicted: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdetected_panels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Actual: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_labels[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Évaluer l'accuracy multi-label sur les données de validation\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m multi_label_accuracy \u001b[38;5;241m=\u001b[39m evaluate_multi_label_accuracy(val_images, val_labels, svm_clf, le)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMulti-label Validation Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmulti_label_accuracy\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[40], line 28\u001b[0m, in \u001b[0;36mevaluate_multi_label_accuracy\u001b[1;34m(images, true_labels, svm_clf, le)\u001b[0m\n\u001b[0;32m     26\u001b[0m total_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(images)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image, true_label_set \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(images, true_labels):\n\u001b[1;32m---> 28\u001b[0m     detected_panels \u001b[38;5;241m=\u001b[39m detect_panels_in_image(image, svm_clf, le)\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m detected_panels \u001b[38;5;241m==\u001b[39m true_label_set:\n\u001b[0;32m     30\u001b[0m         correct_predictions \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[41], line 10\u001b[0m, in \u001b[0;36mdetect_panels_in_image\u001b[1;34m(image, svm_clf, le, window_size, step_size)\u001b[0m\n\u001b[0;32m      8\u001b[0m window_image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(window)\u001b[38;5;241m.\u001b[39mresize((\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m))  \u001b[38;5;66;03m# Resize window to 256x256\u001b[39;00m\n\u001b[0;32m      9\u001b[0m window_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(window_image)\n\u001b[1;32m---> 10\u001b[0m features, _ \u001b[38;5;241m=\u001b[39m hog(window_np, orientations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m9\u001b[39m, pixels_per_cell\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m),\n\u001b[0;32m     11\u001b[0m                   cells_per_block\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m), block_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL2-Hys\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     12\u001b[0m                   visualize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, transform_sqrt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     13\u001b[0m features \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     14\u001b[0m predictions \u001b[38;5;241m=\u001b[39m svm_clf\u001b[38;5;241m.\u001b[39mpredict(features)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\skimage\\_shared\\utils.py:316\u001b[0m, in \u001b[0;36mchannel_as_last_axis.__call__.<locals>.fixed_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    313\u001b[0m channel_axis \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannel_axis\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m channel_axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;66;03m# TODO: convert scalars to a tuple in anticipation of eventually\u001b[39;00m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;66;03m#       supporting a tuple of channel axes. Right now, only an\u001b[39;00m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;66;03m#       integer or a single-element tuple is supported, though.\u001b[39;00m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(channel_axis):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\skimage\\feature\\_hog.py:260\u001b[0m, in \u001b[0;36mhog\u001b[1;34m(image, orientations, pixels_per_cell, cells_per_block, block_norm, visualize, transform_sqrt, feature_vector, channel_axis)\u001b[0m\n\u001b[0;32m    254\u001b[0m                 centre \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m([r \u001b[38;5;241m*\u001b[39m c_row \u001b[38;5;241m+\u001b[39m c_row \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    255\u001b[0m                                 c \u001b[38;5;241m*\u001b[39m c_col \u001b[38;5;241m+\u001b[39m c_col \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m    256\u001b[0m                 rr, cc \u001b[38;5;241m=\u001b[39m draw\u001b[38;5;241m.\u001b[39mline(\u001b[38;5;28mint\u001b[39m(centre[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m dc),\n\u001b[0;32m    257\u001b[0m                                    \u001b[38;5;28mint\u001b[39m(centre[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m dr),\n\u001b[0;32m    258\u001b[0m                                    \u001b[38;5;28mint\u001b[39m(centre[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m dc),\n\u001b[0;32m    259\u001b[0m                                    \u001b[38;5;28mint\u001b[39m(centre[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m dr))\n\u001b[1;32m--> 260\u001b[0m                 hog_image[rr, cc] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m orientation_histogram[r, c, o]\n\u001b[0;32m    262\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;124;03mThe fourth stage computes normalization, which takes local groups of\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;124;03mcells and contrast normalizes their overall responses before passing\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;124;03mGradient (HOG) descriptors.\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    277\u001b[0m n_blocks_row \u001b[38;5;241m=\u001b[39m (n_cells_row \u001b[38;5;241m-\u001b[39m b_row) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def load_data(images_path, labels_path):\n",
    "    \"\"\"Load images and labels from the specified directories.\"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    image_files = [f for f in os.listdir(images_path) if f.endswith('.jpg')]\n",
    "    for image_file in image_files:\n",
    "        image_path = os.path.join(images_path, image_file)\n",
    "        label_file = os.path.join(labels_path, os.path.splitext(image_file)[0] + '.csv')\n",
    "\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        with open(label_file, 'r') as csvfile:\n",
    "            csvreader = csv.reader(csvfile)\n",
    "            panel_labels = set()\n",
    "            for row in csvreader:\n",
    "                if len(row) > 4 and row[4] != 'empty':  # Skip empty labels and check for valid length\n",
    "                    panel_labels.add(row[4])\n",
    "            if panel_labels:  # Only add images that have labels\n",
    "                images.append(image)\n",
    "                labels.append(panel_labels)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "def detect_panels_in_image(image, svm_clf, le, window_size=(256, 256), step_size=128):\n",
    "    detected_panels = set()\n",
    "    image = image.convert('L')  # Convert to grayscale\n",
    "    image_np = np.array(image)\n",
    "    for y in range(0, image_np.shape[0] - window_size[1], step_size):\n",
    "        for x in range(0, image_np.shape[1] - window_size[0], step_size):\n",
    "            window = image_np[y:y + window_size[1], x:x + window_size[0]]\n",
    "            window_image = Image.fromarray(window).resize((256, 256))  # Resize window to 256x256\n",
    "            window_np = np.array(window_image)\n",
    "            features, _ = hog(window_np, orientations=9, pixels_per_cell=(8, 8),\n",
    "                              cells_per_block=(2, 2), block_norm='L2-Hys',\n",
    "                              visualize=True, transform_sqrt=True)\n",
    "            features = features.reshape(1, -1)\n",
    "            predictions = svm_clf.predict(features)\n",
    "            predicted_label = le.inverse_transform(predictions)\n",
    "            if predicted_label[0] != 'empty':  # Assume 'empty' means no panel\n",
    "                detected_panels.add(predicted_label[0])\n",
    "    return detected_panels\n",
    "# Charger les données de validation\n",
    "val_images_path = 'val/images'\n",
    "val_labels_path = 'val/labels'\n",
    "val_images, val_labels = load_data(val_images_path, val_labels_path)\n",
    "\n",
    "# Afficher les prédictions pour les 5 premières images de validation\n",
    "for i in range(min(5, len(val_images))):\n",
    "    detected_panels = detect_panels_in_image(val_images[i], svm_clf, le)\n",
    "    print(f\"Image {i+1} - Predicted: {detected_panels}, Actual: {val_labels[i]}\")\n",
    "\n",
    "# Évaluer l'accuracy multi-label sur les données de validation\n",
    "multi_label_accuracy = evaluate_multi_label_accuracy(val_images, val_labels, svm_clf, le)\n",
    "print(f\"Multi-label Validation Accuracy: {multi_label_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "586905d2-b80c-4298-b95e-9656f6e8486e",
   "metadata": {},
   "source": [
    "import os\n",
    "import csv\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image, ImageEnhance, ImageOps\n",
    "\n",
    "def add_noise(img, noise_level=0.05):\n",
    "    \"\"\"Add random noise to an image.\"\"\"\n",
    "    np_img = np.array(img)\n",
    "    noise = np.random.normal(loc=0, scale=noise_level, size=np_img.shape)\n",
    "    np_img = np.clip(np_img + noise * 255, 0, 255).astype(np.uint8)\n",
    "    return Image.fromarray(np_img)\n",
    "\n",
    "def adjust_and_save_image(image_path, csv_path, new_image_path, new_csv_path, augmentation_type):\n",
    "    \"\"\"Create an adjusted image and save it with a new label CSV.\"\"\"\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "    # Apply the specified augmentation\n",
    "    img = add_noise(img)\n",
    "    img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "    img.save(new_image_path)\n",
    "\n",
    "    # Copy the CSV file with the new name\n",
    "    shutil.copy(csv_path, new_csv_path)\n",
    "\n",
    "def balance_dataset(images_path, labels_path, new_images_path, target_count=200):\n",
    "    # Create new images and labels directories if they don't exist\n",
    "    new_images_dir = os.path.join(new_images_path, 'images')\n",
    "    new_labels_dir = os.path.join(new_images_path, 'labels')\n",
    "    os.makedirs(new_images_dir, exist_ok=True)\n",
    "    os.makedirs(new_labels_dir, exist_ok=True)\n",
    "\n",
    "    # Count the number of each type of panel\n",
    "    panneaux_count = {}\n",
    "    image_files = [f for f in os.listdir(images_path) if f.endswith('.jpg')]\n",
    "\n",
    "    # Map image files to their respective classes\n",
    "    image_class_map = {}\n",
    "\n",
    "    for image_file in image_files:\n",
    "        base_name = os.path.splitext(image_file)[0]\n",
    "        label_file = os.path.join(labels_path, base_name + '.csv')\n",
    "\n",
    "        if os.path.exists(label_file) and os.path.getsize(label_file) > 0:\n",
    "            with open(label_file, 'r') as csvfile:\n",
    "                csvreader = csv.reader(csvfile)\n",
    "                for row in csvreader:\n",
    "                    if row:\n",
    "                        class_name = row[4]\n",
    "                        if class_name in panneaux_count:\n",
    "                            panneaux_count[class_name] += 1\n",
    "                        else:\n",
    "                            panneaux_count[class_name] = 1\n",
    "                        \n",
    "                        if class_name not in image_class_map:\n",
    "                            image_class_map[class_name] = []\n",
    "                        image_class_map[class_name].append(image_file)\n",
    "\n",
    "    # Determine how many images to create for each panel type\n",
    "    augmentation_types = ['flip', 'noise']\n",
    "    for class_name, count in panneaux_count.items():\n",
    "        if class_name == 'interdiction':\n",
    "            continue\n",
    "        if class_name== 'empty':\n",
    "            continue\n",
    "\n",
    "        images_needed = target_count - count\n",
    "        if images_needed > 0:\n",
    "            current_images = image_class_map.get(class_name, [])\n",
    "            if not current_images:\n",
    "                continue\n",
    "\n",
    "            for i in range(images_needed):\n",
    "                image_to_augment = random.choice(current_images)\n",
    "                base_name = os.path.splitext(image_to_augment)[0]\n",
    "                image_path = os.path.join(images_path, image_to_augment)\n",
    "                csv_path = os.path.join(labels_path, base_name + '.csv')\n",
    "\n",
    "                new_base_name = f\"{base_name}_aug_{i}\"\n",
    "                new_image_path = os.path.join(new_images_dir, new_base_name + '.jpg')\n",
    "                new_csv_path = os.path.join(new_labels_dir, new_base_name + '.csv')\n",
    "\n",
    "                augmentation_type = random.choice(augmentation_types)\n",
    "                adjust_and_save_image(image_path, csv_path, new_image_path, new_csv_path, augmentation_type)\n",
    "\n",
    "    # Copy original images and labels to the new directories\n",
    "    for image_file in image_files:\n",
    "        base_name = os.path.splitext(image_file)[0]\n",
    "        image_path = os.path.join(images_path, image_file)\n",
    "        csv_path = os.path.join(labels_path, base_name + '.csv')\n",
    "\n",
    "        new_image_path = os.path.join(new_images_dir, image_file)\n",
    "        new_csv_path = os.path.join(new_labels_dir, base_name + '.csv')\n",
    "\n",
    "        shutil.copy(image_path, new_image_path)\n",
    "        shutil.copy(csv_path, new_csv_path)\n",
    "\n",
    "# Définir les chemins\n",
    "images_path = 'train/images'\n",
    "labels_path = 'train/labels'\n",
    "new_images_path = 'Newimages'\n",
    "\n",
    "# Appeler la fonction pour équilibrer le dataset\n",
    "balance_dataset(images_path, labels_path, new_images_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75f58f3-be02-42a2-8213-287f50e2450c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import joblib\n",
    "from skimage import io, color\n",
    "from skimage.feature import hog\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from joblib import Parallel, delayed\n",
    "from skimage.transform import resize\n",
    "\n",
    "# Chemins vers les dossiers\n",
    "new_images_path = 'Newimages'\n",
    "images_path = os.path.join(new_images_path, 'images')\n",
    "labels_path = os.path.join(new_images_path, 'labels')\n",
    "\n",
    "# Paramètres HOG\n",
    "hog_params = {\n",
    "    'orientations': 9,\n",
    "    'pixels_per_cell': (8, 8),\n",
    "    'cells_per_block': (2, 2),\n",
    "    'block_norm': 'L2-Hys'\n",
    "}\n",
    "\n",
    "def extract_hog_features(image_path):\n",
    "    # Lire l'image\n",
    "    image = io.imread(image_path)\n",
    "    # Convertir l'image en niveaux de gris\n",
    "    gray_image = color.rgb2gray(image)\n",
    "    # Redimensionner l'image pour une taille fixe (optionnel)\n",
    "    resized_image = resize(gray_image, (128, 128))\n",
    "    # Extraire les caractéristiques HOG\n",
    "    hog_features = hog(resized_image, **hog_params)\n",
    "    return hog_features\n",
    "\n",
    "def load_data_parallel(images_path, labels_path, n_jobs=-1):\n",
    "    image_files = [f for f in os.listdir(images_path) if f.endswith('.jpg')]\n",
    "    \n",
    "    def process_image(image_file):\n",
    "        base_name = os.path.splitext(image_file)[0]\n",
    "        label_file = os.path.join(labels_path, base_name + '.csv')\n",
    "        image_path = os.path.join(images_path, image_file)\n",
    "        \n",
    "        if os.path.exists(label_file):\n",
    "            with open(label_file, 'r') as csvfile:\n",
    "                csvreader = csv.reader(csvfile)\n",
    "                labels = [row for row in csvreader if row]\n",
    "                if labels:\n",
    "                    for row in labels:\n",
    "                        if len(row) >= 5:  # Vérifie que la ligne a au moins 5 éléments\n",
    "                            class_name = row[4]\n",
    "                            if class_name[0] != \"empty\":\n",
    "                                # Extraire les caractéristiques HOG\n",
    "                                hog_features = extract_hog_features(image_path)\n",
    "                                return hog_features, class_name\n",
    "        return None\n",
    "    \n",
    "    results = Parallel(n_jobs=n_jobs)(delayed(process_image)(image_file) for image_file in image_files)\n",
    "    results = [r for r in results if r is not None]\n",
    "    \n",
    "    if not results:\n",
    "        raise ValueError(\"Aucune donnée valide n'a été trouvée. Vérifiez les fichiers d'entrée.\")\n",
    "\n",
    "    features, labels = zip(*results)\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# Charger les données en parallèle\n",
    "features, labels = load_data_parallel(images_path, labels_path)\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Entraîner le classifieur SVM\n",
    "svm_clf = svm.SVC(kernel='linear', probability=True)\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "# Prédire sur l'ensemble de test\n",
    "y_pred = svm_clf.predict(X_test)\n",
    "\n",
    "# Évaluer les performances du classifieur\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Sauvegarder le modèle entraîné\n",
    "joblib.dump(svm_clf, 'svm_hog_classifier.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bc31d5-4984-41bb-b404-325f81281e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "from skimage import io\n",
    "from skimage.feature import hog\n",
    "from joblib import Parallel, delayed\n",
    "from skimage.transform import resize\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Chemins vers les dossiers\n",
    "val_images_path = os.path.join('val', 'images')\n",
    "val_labels_path = os.path.join('val', 'labels')\n",
    "model_path = 'svm_hog_classifier.pkl'\n",
    "\n",
    "# Paramètres HOG (doivent être les mêmes que ceux utilisés pendant l'entraînement)\n",
    "hog_params = {\n",
    "    'orientations': 9,\n",
    "    'pixels_per_cell': (8, 8),\n",
    "    'cells_per_block': (2, 2),\n",
    "    'block_norm': 'L2-Hys'\n",
    "}\n",
    "\n",
    "# Taille fixe des images utilisées pour l'extraction des caractéristiques HOG\n",
    "image_size = (128, 128)\n",
    "\n",
    "def extract_hog_features(image_path):\n",
    "    # Lire l'image\n",
    "    image = io.imread(image_path)\n",
    "    # Redimensionner l'image pour une taille fixe (doit être la même que celle utilisée pendant l'entraînement)\n",
    "    resized_image = resize(image, image_size)\n",
    "    # Extraire les caractéristiques HOG en spécifiant l'axe des canaux\n",
    "    hog_features = hog(resized_image, channel_axis=-1, **hog_params)\n",
    "    return hog_features\n",
    "\n",
    "def load_val_data(images_path, labels_path, n_jobs=-1):\n",
    "    image_files = [f for f in os.listdir(images_path) if f.endswith('.jpg')]\n",
    "    \n",
    "    def process_image(image_file):\n",
    "        base_name = os.path.splitext(image_file)[0]\n",
    "        label_file = os.path.join(labels_path, base_name + '.csv')\n",
    "        image_path = os.path.join(images_path, image_file)\n",
    "        \n",
    "        if os.path.exists(label_file):\n",
    "            with open(label_file, 'r') as csvfile:\n",
    "                csvreader = csv.reader(csvfile)\n",
    "                labels = [row for row in csvreader if row]\n",
    "                if labels:\n",
    "                    for row in labels:\n",
    "                        if len(row) >= 5:  # Vérifie que la ligne a au moins 5 éléments\n",
    "                            class_name = row[4]\n",
    "                            if class_name != \"empty\":\n",
    "                                # Extraire les caractéristiques HOG\n",
    "                                hog_features = extract_hog_features(image_path)\n",
    "                                return hog_features, class_name, image_file\n",
    "        return None\n",
    "    \n",
    "    results = Parallel(n_jobs=n_jobs)(delayed(process_image)(image_file) for image_file in image_files)\n",
    "    results = [r for r in results if r is not None]\n",
    "    \n",
    "    if not results:\n",
    "        raise ValueError(\"Aucune donnée valide n'a été trouvée. Vérifiez les fichiers d'entrée.\")\n",
    "\n",
    "    features, labels, image_files = zip(*results)\n",
    "    return np.array(features), np.array(labels), image_files\n",
    "\n",
    "# Charger les données de validation\n",
    "val_features, val_labels, val_image_files = load_val_data(val_images_path, val_labels_path)\n",
    "\n",
    "# Charger le modèle SVM entraîné\n",
    "svm_clf = joblib.load(model_path)\n",
    "\n",
    "# Prédire sur le dataset de validation\n",
    "val_predictions = svm_clf.predict(val_features)\n",
    "\n",
    "# Calculer le taux de bons résultats\n",
    "accuracy = accuracy_score(val_labels, val_predictions)\n",
    "print(f\"Taux de bon résultat : {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Afficher le rapport de classification\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(val_labels, val_predictions))\n",
    "\n",
    "# Afficher 5 noms d'images avec la classe prédite\n",
    "print(\"5 images avec leur classe prédite :\")\n",
    "for i in range(min(5, len(val_image_files))):\n",
    "    print(f\"Image: {val_image_files[i]}, Classe réelle: {val_labels[i]}, Classe prédite: {val_predictions[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c546e1d7-6889-4400-9d21-218f687ac8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "from skimage import io\n",
    "from skimage.feature import hog\n",
    "from joblib import Parallel, delayed\n",
    "from skimage.transform import resize\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Chemins vers les dossiers\n",
    "val_images_path = os.path.join('val', 'images')\n",
    "val_labels_path = os.path.join('val', 'labels')\n",
    "model_path = 'svm_hog_classifier.pkl'\n",
    "\n",
    "# Paramètres HOG (doivent être les mêmes que ceux utilisés pendant l'entraînement)\n",
    "hog_params = {\n",
    "    'orientations': 9,\n",
    "    'pixels_per_cell': (8, 8),\n",
    "    'cells_per_block': (2, 2),\n",
    "    'block_norm': 'L2-Hys'\n",
    "}\n",
    "\n",
    "# Taille fixe des images utilisées pour l'extraction des caractéristiques HOG\n",
    "image_size = (256, 256)\n",
    "\n",
    "def extract_hog_features(image_path):\n",
    "    # Lire l'image\n",
    "    image = io.imread(image_path)\n",
    "    # Redimensionner l'image pour une taille fixe (doit être la même que celle utilisée pendant l'entraînement)\n",
    "    resized_image = resize(image, image_size)\n",
    "    # Extraire les caractéristiques HOG en spécifiant l'axe des canaux\n",
    "    hog_features = hog(resized_image, channel_axis=-1, **hog_params)\n",
    "    return hog_features\n",
    "\n",
    "def load_val_data(images_path, labels_path, n_jobs=-1):\n",
    "    image_files = [f for f in os.listdir(images_path) if f.endswith('.jpg')]\n",
    "    \n",
    "    def process_image(image_file):\n",
    "        base_name = os.path.splitext(image_file)[0]\n",
    "        label_file = os.path.join(labels_path, base_name + '.csv')\n",
    "        image_path = os.path.join(images_path, image_file)\n",
    "        \n",
    "        if os.path.exists(label_file):\n",
    "            with open(label_file, 'r') as csvfile:\n",
    "                csvreader = csv.reader(csvfile)\n",
    "                labels = [row for row in csvreader if row]\n",
    "                if labels:\n",
    "                    for row in labels:\n",
    "                        if len(row) >= 5:  # Vérifie que la ligne a au moins 5 éléments\n",
    "                            class_name = row[4]\n",
    "                            if class_name != \"empty\":\n",
    "                                # Extraire les caractéristiques HOG\n",
    "                                hog_features = extract_hog_features(image_path)\n",
    "                                return hog_features, class_name, image_file\n",
    "        return None\n",
    "    \n",
    "    results = Parallel(n_jobs=n_jobs)(delayed(process_image)(image_file) for image_file in image_files)\n",
    "    results = [r for r in results if r is not None]\n",
    "    \n",
    "    if not results:\n",
    "        raise ValueError(\"Aucune donnée valide n'a été trouvée. Vérifiez les fichiers d'entrée.\")\n",
    "\n",
    "    features, labels, image_files = zip(*results)\n",
    "    return np.array(features), np.array(labels), image_files\n",
    "\n",
    "# Charger les données de validation\n",
    "val_features, val_labels, val_image_files = load_val_data(val_images_path, val_labels_path)\n",
    "\n",
    "# Charger le modèle SVM entraîné\n",
    "svm_clf = joblib.load(model_path)\n",
    "\n",
    "# Prédire sur le dataset de validation\n",
    "val_predictions = svm_clf.predict(val_features)\n",
    "\n",
    "# Calculer le taux de bons résultats\n",
    "accuracy = accuracy_score(val_labels, val_predictions)\n",
    "print(f\"Taux de bon résultat : {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Afficher le rapport de classification\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(val_labels, val_predictions))\n",
    "\n",
    "# Afficher 5 noms d'images avec la classe prédite\n",
    "print(\"5 images avec leur classe prédite :\")\n",
    "for i in range(min(5, len(val_image_files))):\n",
    "    print(f\"Image: {val_image_files[i]}, Classe réelle: {val_labels[i]}, Classe prédite: {val_predictions[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5cdfd8-9e3b-4730-96ee-dc1b3a55618c",
   "metadata": {},
   "source": [
    "Utilisateur de OneVsRestClassifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
